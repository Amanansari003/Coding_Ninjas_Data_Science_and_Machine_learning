{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1 : Print Body tag**    \n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print all the contents of the body tag(including body tag)\n",
    "*NOTE :*  \n",
    " \n",
    "    You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "print(data.body)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2: Attributes of div tag**   \n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the name of all attributes of first div tag of the page\n",
    "**NOTE :**\n",
    "        \n",
    "        You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_div']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n",
    "\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "dct = data.div.attrs\n",
    "for i in dct :\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3 : Strings of li**   \n",
    "You are given the HTML content of a webpage, your task is to :\n",
    "Print the strings(only text without tag names) of all li tags separated by a space   \n",
    "**NOTE :**  \n",
    "    \n",
    "     You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an unordered list.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "li_tags = soup.find_all('li')\n",
    "result = ' '.join([tag.get_text() for tag in li_tags])\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4 : href of A tag**   \n",
    "You are given the HTML content of a webpage, your task is to :    \n",
    "Print the href of all the < a > tags on the page in different lines   \n",
    "*NOTE :*\n",
    "\n",
    "     You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.codingninjas.in/\n",
      "https://www.facebook.com/codingninjas/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "li_tags = soup.find_all('a')\n",
    "##BOTH ARE THE POSIBLE WAY TOP GET THE OUTPUT \n",
    "# print(li_tags[0].attrs)\n",
    "# print(li_tags[0].get(\"href\"))\n",
    "for tag in li_tags:\n",
    "    print(tag.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div>\n",
      "<body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body>\n",
      "<html><head><title>Learning Beautiful Soup</title></head><body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body></html>\n",
      "<!DOCTYPE html>\n",
      "<html><head><title>Learning Beautiful Soup</title></head><body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body></html>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "data  = BeautifulSoup(html, 'html.parser')\n",
    "# print(data.prettify())\n",
    "l = list(data.p.parents)\n",
    "for i in l :\n",
    "    print(i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5: Descendants and children**  \n",
    "You are given the HTML content of a webpage, your task is to :  \n",
    "Print the difference between the number of descendants and the number of children of the html tag  \n",
    "*NOTE :*\n",
    "\n",
    "     You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "## HTML Code is provided in variable html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "data  = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "a = len(data.html.contents)\n",
    "b = len(list(data.html.descendants))\n",
    "print(b-a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6 : Name of tags with ID**  \n",
    "\n",
    "You are given the HTML content of a webpage, your task is to :  \n",
    "Print the name of all the tags in different lines that have an id attribute.  \n",
    "*NOTE :* \n",
    "\n",
    "    You are provided the HTML content inside variable html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "## HTML Code is provided in variable html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "data  = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for element in data.find_all(id=True):\n",
    "    print(element.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7: Next Sibling**   \n",
    "You are given the HTML content of a webpage, your task is to :  \n",
    "Print all content of the next siblings of the tag that have id as “li2”(in different lines)  \n",
    "**NOTE :**\n",
    "\n",
    "     Content includes the complete html of tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li>\n",
      "<li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li>\n"
     ]
    }
   ],
   "source": [
    "## HTML Code is provided in variable html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "# data  = BeautifulSoup(html, 'html.parser')\n",
    "# print(data.prettify())\n",
    "# l = data.find_all(id=\"li2\")\n",
    "# # print(l)\n",
    "# print(l[0].next_sibling)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "li2_tag = soup.find('li', {'id': 'li2'})\n",
    "# print(li2_tag.next_sibling)\n",
    "# print(list(li2_tag.next_siblings))\n",
    "\n",
    "for sibling in li2_tag.find_next_siblings():\n",
    "    print(sibling) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8 :**\n",
    "You are given the HTML content of a webpage, your task is to :  \n",
    "Print content of all the parents of the title tag(linewise)  \n",
    "NOTE : Content includes the whole HTML of the tag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>Navigate Parse Tree</title></head>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n",
      "<!DOCTYPE html>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "data = BeautifulSoup(html,'html.parser')\n",
    "l = data.title.parents\n",
    "for i in l :\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9 : Next Element**  \n",
    "You are given the HTML content of a webpage, your task is to :  \n",
    "Print the string which is present inside the isecond < a > tag using BeautifulSoup's next_element property\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicking on this takes you to the documentation of BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'\n",
    "\n",
    "data = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "### collectiing all the anchore tag from the html content and taking only the second one from the list of anchore tag that why used the [1] indexing for list \n",
    "second = data.find_all('a')[1]\n",
    "print(second.next_element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 10 : Book Names from First Page**  \n",
    "Print the title of all 20 books which are present on first page of <a href= \"https://books.toscrape.com\"> this <a> website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://books.toscrape.com\")\n",
    "\n",
    "html = response.text\n",
    "data = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "book_title = data.find_all(class_ = \"product_pod\")\n",
    "for b1 in book_title : \n",
    "    print(b1.h3.a[\"title\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 11 :All Categories**  \n",
    "Print the name of all categories which are present https://books.toscrape.com website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "response = requests.get(\"https://books.toscrape.com\")\n",
    "\n",
    "html = response.text\n",
    "\n",
    "data = BeautifulSoup(html,\"html.parser\")\n",
    "catogories = data.find(class_=\"nav nav-list\")\n",
    "all_link = catogories.find_all(\"a\")\n",
    "# print(len(all_link))\n",
    "for i in all_link :\n",
    "    if i.string.strip() != 'Books' :\n",
    "        print(i.string.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12 All Book Names**   \n",
    "Print the title of all books which are present on first 10 pages of https://books.toscrape.com website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "allPages = [\"https://books.toscrape.com/catalogue/page-1.html\"]\n",
    "current_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "response = requests.get(current_url)\n",
    "\n",
    "while response.status_code == 200:\n",
    "    data = BeautifulSoup(response.text,\"html.parser\")\n",
    "    next_page = data.find(class_ = \"next\")\n",
    "    if next_page is None :\n",
    "        break\n",
    "    next_page = base_url+next_page.a[\"href\"]\n",
    "    allPages.append(next_page)\n",
    "    current_url = next_page \n",
    "    response = requests.get(current_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n",
      "In Her Wake\n",
      "How Music Works\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More\n",
      "Chase Me (Paris Nights #2)\n",
      "Black Dust\n",
      "Birdsong: A Story in Pictures\n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana\n",
      "Aladdin and His Wonderful Lamp\n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe\n",
      "Wall and Piece\n",
      "The Four Agreements: A Practical Guide to Personal Freedom\n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate\n",
      "The Elephant Tree\n",
      "The Bear and the Piano\n",
      "Sophie's World\n",
      "Penny Maybe\n",
      "Maude (1883-1993):She Grew Up with the country\n",
      "In a Dark, Dark Wood\n",
      "Behind Closed Doors\n",
      "You can't bury them all: Poems\n",
      "Slow States of Collapse: Poems\n",
      "Reasons to Stay Alive\n",
      "Private Paris (Private #10)\n",
      "#HigherSelfie: Wake Up Your Life. Free Your Soul. Find Your Tribe.\n",
      "Without Borders (Wanderlove #1)\n",
      "When We Collided\n",
      "We Love You, Charlie Freeman\n",
      "Untitled Collection: Sabbath Poems 2014\n",
      "Unseen City: The Majesty of Pigeons, the Discreet Charm of Snails & Other Wonders of the Urban Wilderness\n",
      "Unicorn Tracks\n",
      "Unbound: How Eight Technologies Made Us Human, Transformed Society, and Brought Our World to the Brink\n",
      "Tsubasa: WoRLD CHRoNiCLE 2 (Tsubasa WoRLD CHRoNiCLE #2)\n",
      "Throwing Rocks at the Google Bus: How Growth Became the Enemy of Prosperity\n",
      "This One Summer\n",
      "Thirst\n",
      "The Torch Is Passed: A Harding Family Story\n",
      "The Secret of Dreadwillow Carse\n",
      "The Pioneer Woman Cooks: Dinnertime: Comfort Classics, Freezer Food, 16-Minute Meals, and Other Delicious Ways to Solve Supper!\n",
      "The Past Never Ends\n",
      "The Natural History of Us (The Fine Art of Pretending #2)\n",
      "The Nameless City (The Nameless City #1)\n",
      "The Murder That Never Was (Forensic Instincts #5)\n",
      "The Most Perfect Thing: Inside (and Outside) a Bird's Egg\n",
      "The Mindfulness and Acceptance Workbook for Anxiety: A Guide to Breaking Free from Anxiety, Phobias, and Worry Using Acceptance and Commitment Therapy\n",
      "The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing\n",
      "The Inefficiency Assassin: Time Management Tactics for Working Smarter, Not Longer\n",
      "The Gutsy Girl: Escapades for Your Life of Epic Adventure\n",
      "The Electric Pencil: Drawings from Inside State Hospital No. 3\n",
      "The Death of Humanity: and the Case for Life\n",
      "The Bulletproof Diet: Lose up to a Pound a Day, Reclaim Energy and Focus, Upgrade Your Life\n",
      "The Art Forger\n",
      "Princess Jellyfish 2-in-1 Omnibus, Vol. 01 (Princess Jellyfish 2-in-1 Omnibus #1)\n",
      "Princess Between Worlds (Wide-Awake Princess #5)\n",
      "Pop Gun War, Volume 1: Gift\n",
      "Political Suicide: Missteps, Peccadilloes, Bad Calls, Backroom Hijinx, Sordid Pasts, Rotten Breaks, and Just Plain Dumb Mistakes in the Annals of American Politics\n",
      "Patience\n",
      "Outcast, Vol. 1: A Darkness Surrounds Him (Outcast #1)\n",
      "orange: The Complete Collection 1 (orange: The Complete Collection #1)\n",
      "Online Marketing for Busy Authors: A Step-By-Step Guide\n",
      "On a Midnight Clear\n",
      "Obsidian (Lux #1)\n",
      "My Paris Kitchen: Recipes and Stories\n",
      "Masks and Shadows\n",
      "Mama Tried: Traditional Italian Cooking for the Screwed, Crude, Vegan, and Tattooed\n",
      "Lumberjanes, Vol. 2: Friendship to the Max (Lumberjanes #5-8)\n",
      "Lumberjanes, Vol. 1: Beware the Kitten Holy (Lumberjanes #1-4)\n",
      "Lumberjanes Vol. 3: A Terrible Plan (Lumberjanes #9-12)\n",
      "Layered: Baking, Building, and Styling Spectacular Cakes\n",
      "Judo: Seven Steps to Black Belt (an Introductory Guide for Beginners)\n",
      "Join\n",
      "In the Country We Love: My Family Divided\n",
      "Immunity: How Elie Metchnikoff Changed the Course of Modern Medicine\n",
      "I Hate Fairyland, Vol. 1: Madly Ever After (I Hate Fairyland (Compilations) #1-5)\n",
      "I am a Hero Omnibus Volume 1\n",
      "How to Be Miserable: 40 Strategies You Already Use\n",
      "Her Backup Boyfriend (The Sorensen Family #1)\n",
      "Giant Days, Vol. 2 (Giant Days #5-8)\n",
      "Forever and Forever: The Courtship of Henry Longfellow and Fanny Appleton\n",
      "First and First (Five Boroughs #3)\n",
      "Fifty Shades Darker (Fifty Shades #2)\n",
      "Everydata: The Misinformation Hidden in the Little Data You Consume Every Day\n",
      "Don't Be a Jerk: And Other Practical Advice from Dogen, Japan's Greatest Zen Master\n",
      "Danganronpa Volume 1\n",
      "Crown of Midnight (Throne of Glass #2)\n",
      "Codename Baboushka, Volume 1: The Conclave of Death\n",
      "Camp Midnight\n",
      "Call the Nurse: True Stories of a Country Nurse on a Scottish Isle\n",
      "Burning\n",
      "Bossypants\n",
      "Bitch Planet, Vol. 1: Extraordinary Machine (Bitch Planet (Collected Editions))\n",
      "Avatar: The Last Airbender: Smoke and Shadow, Part 3 (Smoke and Shadow #3)\n",
      "Algorithms to Live By: The Computer Science of Human Decisions\n",
      "A World of Flavor: Your Gluten Free Passport\n",
      "A Piece of Sky, a Grain of Rice: A Memoir in Four Meditations\n",
      "A Murder in Time\n",
      "A Flight of Arrows (The Pathfinders #2)\n",
      "A Fierce and Subtle Poison\n",
      "A Court of Thorns and Roses (A Court of Thorns and Roses #1)\n",
      "(Un)Qualified: How God Uses Broken People to Do Big Things\n",
      "You Are What You Love: The Spiritual Power of Habit\n",
      "William Shakespeare's Star Wars: Verily, A New Hope (William Shakespeare's Star Wars #4)\n",
      "Tuesday Nights in 1980\n",
      "Tracing Numbers on a Train\n",
      "Throne of Glass (Throne of Glass #1)\n",
      "Thomas Jefferson and the Tripoli Pirates: The Forgotten War That Changed American History\n",
      "Thirteen Reasons Why\n",
      "The White Cat and the Monk: A Retelling of the Poem âPangur BÃ¡nâ\n",
      "The Wedding Dress\n",
      "The Vacationers\n",
      "The Third Wave: An Entrepreneurâs Vision of the Future\n",
      "The Stranger\n",
      "The Shadow Hero (The Shadow Hero)\n",
      "The Secret (The Secret #1)\n",
      "The Regional Office Is Under Attack!\n",
      "The Psychopath Test: A Journey Through the Madness Industry\n",
      "The Project\n",
      "The Power of Now: A Guide to Spiritual Enlightenment\n",
      "The Omnivore's Dilemma: A Natural History of Four Meals\n",
      "The Nerdy Nummies Cookbook: Sweet Treats for the Geek in All of Us\n",
      "The Murder of Roger Ackroyd (Hercule Poirot #4)\n",
      "The Mistake (Off-Campus #2)\n",
      "The Matchmaker's Playbook (Wingmen Inc. #1)\n",
      "The Love and Lemons Cookbook: An Apple-to-Zucchini Celebration of Impromptu Cooking\n",
      "The Long Shadow of Small Ghosts: Murder and Memory in an American City\n",
      "The Kite Runner\n",
      "The House by the Lake\n",
      "The Glittering Court (The Glittering Court #1)\n",
      "The Girl on the Train\n",
      "The Genius of Birds\n",
      "The Emerald Mystery\n",
      "The Cookies & Cups Cookbook: 125+ sweet & savory recipes reminding you to Always Eat Dessert First\n",
      "The Bridge to Consciousness: I'm Writing the Bridge Between Science and Our Old and New Beliefs.\n",
      "The Artist's Way: A Spiritual Path to Higher Creativity\n",
      "The Art of War\n",
      "The Argonauts\n",
      "The 10% Entrepreneur: Live Your Startup Dream Without Quitting Your Day Job\n",
      "Suddenly in Love (Lake Haven #1)\n",
      "Something More Than This\n",
      "Soft Apocalypse\n",
      "So You've Been Publicly Shamed\n",
      "Shoe Dog: A Memoir by the Creator of NIKE\n",
      "Shobu Samurai, Project Aryoku (#3)\n",
      "Secrets and Lace (Fatal Hearts #1)\n",
      "Scarlett Epstein Hates It Here\n",
      "Romero and Juliet: A Tragic Tale of Love and Zombies\n",
      "Redeeming Love\n",
      "Poses for Artists Volume 1 - Dynamic and Sitting Poses: An Essential Reference for Figure Drawing and the Human Form\n",
      "Poems That Make Grown Women Cry\n",
      "Nightingale, Sing\n",
      "Night Sky with Exit Wounds\n",
      "Mrs. Houdini\n",
      "Modern Romance\n",
      "Miss Peregrineâs Home for Peculiar Children (Miss Peregrineâs Peculiar Children #1)\n",
      "Louisa: The Extraordinary Life of Mrs. Adams\n",
      "Little Red\n",
      "Library of Souls (Miss Peregrineâs Peculiar Children #3)\n",
      "Large Print Heart of the Pride\n",
      "I Had a Nice Time And Other Lies...: How to find love & sh*t like that\n",
      "Hollow City (Miss Peregrineâs Peculiar Children #2)\n",
      "Grumbles\n",
      "Full Moon over Noahâs Ark: An Odyssey to Mount Ararat and Beyond\n",
      "Frostbite (Vampire Academy #2)\n",
      "Follow You Home\n",
      "First Steps for New Christians (Print Edition)\n",
      "Finders Keepers (Bill Hodges Trilogy #2)\n",
      "Fables, Vol. 1: Legends in Exile (Fables #1)\n",
      "Eureka Trivia 6.0\n",
      "Drive: The Surprising Truth About What Motivates Us\n",
      "Done Rubbed Out (Reightman & Bailey #1)\n",
      "Doing It Over (Most Likely To #1)\n",
      "Deliciously Ella Every Day: Quick and Easy Recipes for Gluten-Free Snacks, Packed Lunches, and Simple Meals\n",
      "Dark Notes\n",
      "Daring Greatly: How the Courage to Be Vulnerable Transforms the Way We Live, Love, Parent, and Lead\n",
      "Close to You\n",
      "Chasing Heaven: What Dying Taught Me About Living\n",
      "Big Magic: Creative Living Beyond Fear\n",
      "Becoming Wise: An Inquiry into the Mystery and Art of Living\n",
      "Beauty Restored (Riley Family Legacy Novellas #3)\n",
      "Batman: The Long Halloween (Batman)\n",
      "Batman: The Dark Knight Returns (Batman)\n",
      "Ayumi's Violin\n",
      "Anonymous\n",
      "Amy Meets the Saints and Sages\n",
      "Amid the Chaos\n",
      "Amatus\n",
      "Agnostic: A Spirited Manifesto\n",
      "Zealot: The Life and Times of Jesus of Nazareth\n",
      "You (You #1)\n",
      "Wonder Woman: Earth One, Volume One (Wonder Woman: Earth One #1)\n",
      "Wild Swans\n",
      "Why the Right Went Wrong: Conservatism--From Goldwater to the Tea Party and Beyond\n",
      "Whole Lotta Creativity Going On: 60 Fun and Unusual Exercises to Awaken and Strengthen Your Creativity\n",
      "What's It Like in Space?: Stories from Astronauts Who've Been There\n",
      "We Are Robin, Vol. 1: The Vigilante Business (We Are Robin #1)\n",
      "Walt Disney's Alice in Wonderland\n",
      "V for Vendetta (V for Vendetta Complete)\n",
      "Until Friday Night (The Field Party #1)\n",
      "Unbroken: A World War II Story of Survival, Resilience, and Redemption\n",
      "Twenty Yawns\n",
      "Through the Woods\n",
      "This Is Where It Ends\n",
      "The Year of Magical Thinking\n",
      "The Wright Brothers\n",
      "The White Queen (The Cousins' War #1)\n",
      "The Wedding Pact (The O'Malleys #2)\n",
      "The Time Keeper\n",
      "The Testament of Mary\n",
      "The Star-Touched Queen\n",
      "The Songs of the Gods\n",
      "The Song of Achilles\n",
      "The Rosie Project (Don Tillman #1)\n",
      "The Power of Habit: Why We Do What We Do in Life and Business\n",
      "The Marriage of Opposites\n",
      "The Lucifer Effect: Understanding How Good People Turn Evil\n",
      "The Long Haul (Diary of a Wimpy Kid #9)\n",
      "The Loney\n",
      "The Literature Book (Big Ideas Simply Explained)\n",
      "The Last Mile (Amos Decker #2)\n",
      "The Immortal Life of Henrietta Lacks\n",
      "The Hidden Oracle (The Trials of Apollo #1)\n",
      "The Help Yourself Cookbook for Kids: 60 Easy Plant-Based Recipes Kids Can Make to Stay Healthy and Save the Earth\n",
      "The Guilty (Will Robie #4)\n",
      "The First Hostage (J.B. Collins #2)\n",
      "The Dovekeepers\n",
      "The Darkest Lie\n",
      "The Bane Chronicles (The Bane Chronicles #1-11)\n",
      "The Bad-Ass Librarians of Timbuktu: And Their Race to Save the Worldâs Most Precious Manuscripts\n",
      "The 14th Colony (Cotton Malone #11)\n",
      "That Darkness (Gardiner and Renner #1)\n",
      "Tastes Like Fear (DI Marnie Rome #3)\n",
      "Take Me with You\n",
      "Swell: A Year of Waves\n",
      "Superman Vol. 1: Before Truth (Superman by Gene Luen Yang #1)\n",
      "Still Life with Bread Crumbs\n",
      "Steve Jobs\n",
      "Sorting the Beef from the Bull: The Science of Food Fraud Forensics\n",
      "Someone Like You (The Harrisons #2)\n",
      "So Cute It Hurts!!, Vol. 6 (So Cute It Hurts!! #6)\n",
      "Shtum\n",
      "See America: A Celebration of Our National Parks & Treasured Sites\n",
      "salt.\n",
      "Robin War\n",
      "Red Hood/Arsenal, Vol. 1: Open for Business (Red Hood/Arsenal #1)\n",
      "Rain Fish\n",
      "Quarter Life Poetry: Poems for the Young, Broke and Hangry\n",
      "Pet Sematary\n",
      "Overload: How to Unplug, Unwind, and Unleash Yourself from the Pressure of Stress\n",
      "Once Was a Time\n",
      "Old School (Diary of a Wimpy Kid #10)\n",
      "No Dream Is Too High: Life Lessons From a Man Who Walked on the Moon\n",
      "Naruto (3-in-1 Edition), Vol. 14: Includes Vols. 40, 41 & 42 (Naruto: Omnibus #14)\n",
      "My Name Is Lucy Barton\n",
      "My Mrs. Brown\n",
      "My Kind of Crazy\n",
      "Mr. Mercedes (Bill Hodges Trilogy #1)\n",
      "More Than Music (Chasing the Dream #1)\n",
      "Made to Stick: Why Some Ideas Survive and Others Die\n",
      "Luis Paints the World\n",
      "Luckiest Girl Alive\n",
      "Lowriders to the Center of the Earth (Lowriders in Space #2)\n",
      "Love Is a Mix Tape (Music #1)\n",
      "Looking for Lovely: Collecting the Moments that Matter\n",
      "Living Leadership by Insight: A Good Leader Achieves, a Great Leader Builds Monuments\n",
      "Let It Out: A Journey Through Journaling\n",
      "Lady Midnight (The Dark Artifices #1)\n",
      "It's All Easy: Healthy, Delicious Weeknight Meals in under 30 Minutes\n",
      "Island of Dragons (Unwanteds #7)\n",
      "I Know What I'm Doing -- and Other Lies I Tell Myself: Dispatches from a Life Under Construction\n",
      "I Am Pilgrim (Pilgrim #1)\n",
      "Hyperbole and a Half: Unfortunate Situations, Flawed Coping Mechanisms, Mayhem, and Other Things That Happened\n",
      "Hush, Hush (Hush, Hush #1)\n",
      "Hold Your Breath (Search and Rescue #1)\n",
      "Hamilton: The Revolution\n",
      "Greek Mythic History\n",
      "God: The Most Unpleasant Character in All Fiction\n",
      "Glory over Everything: Beyond The Kitchen House\n",
      "Feathers: Displays of Brilliant Plumage\n",
      "Far & Away: Places on the Brink of Change: Seven Continents, Twenty-Five Years\n",
      "Every Last Word\n",
      "Eligible (The Austen Project #4)\n",
      "El Deafo\n",
      "Eight Hundred Grapes\n",
      "Eaternity: More than 150 Deliciously Easy Vegan Recipes for a Long, Healthy, Satisfied, Joyful Life\n",
      "Eat Fat, Get Thin\n",
      "Don't Get Caught\n",
      "Doctor Sleep (The Shining #2)\n",
      "Demigods & Magicians: Percy and Annabeth Meet the Kanes (Percy Jackson & Kane Chronicles Crossover #1-3)\n",
      "Dear Mr. Knightley\n",
      "Daily Fantasy Sports\n",
      "Crazy Love: Overwhelmed by a Relentless God\n",
      "Cometh the Hour (The Clifton Chronicles #6)\n",
      "Code Name Verity (Code Name Verity #1)\n",
      "Clockwork Angel (The Infernal Devices #1)\n",
      "City of Glass (The Mortal Instruments #3)\n",
      "City of Fallen Angels (The Mortal Instruments #4)\n",
      "City of Bones (The Mortal Instruments #1)\n",
      "City of Ashes (The Mortal Instruments #2)\n",
      "Cell\n",
      "Catching Jordan (Hundred Oaks)\n",
      "Carry On, Warrior: Thoughts on Life Unarmed\n",
      "Carrie\n",
      "Buying In: The Secret Dialogue Between What We Buy and Who We Are\n",
      "Brain on Fire: My Month of Madness\n",
      "Batman: Europa\n",
      "Barefoot Contessa Back to Basics\n",
      "Barefoot Contessa at Home: Everyday Recipes You'll Make Over and Over Again\n",
      "Balloon Animals\n",
      "Art Ops Vol. 1\n",
      "Aristotle and Dante Discover the Secrets of the Universe (Aristotle and Dante Discover the Secrets of the Universe #1)\n",
      "Angels Walking (Angels Walking #1)\n",
      "Angels & Demons (Robert Langdon #1)\n",
      "All the Light We Cannot See\n",
      "Adulthood Is a Myth: A \"Sarah's Scribbles\" Collection\n",
      "Abstract City\n",
      "A Time of Torment (Charlie Parker #14)\n",
      "A Study in Scarlet (Sherlock Holmes #1)\n",
      "A Series of Catastrophes and Miracles: A True Story of Love, Science, and Cancer\n",
      "A People's History of the United States\n",
      "A Man Called Ove\n",
      "A Distant Mirror: The Calamitous 14th Century\n",
      "A Brush of Wings (Angels Walking #3)\n",
      "1491: New Revelations of the Americas Before Columbus\n",
      "The Three Searches, Meaning, and the Story\n",
      "Searching for Meaning in Gailana\n",
      "Rook\n",
      "My Kitchen Year: 136 Recipes That Saved My Life\n",
      "13 Hours: The Inside Account of What Really Happened In Benghazi\n",
      "Will You Won't You Want Me?\n",
      "Tipping Point for Planet Earth: How Close Are We to the Edge?\n",
      "The Star-Touched Queen\n",
      "The Silent Sister (Riley MacPherson #1)\n",
      "The Midnight Watch: A Novel of the Titanic and the Californian\n",
      "The Lonely City: Adventures in the Art of Being Alone\n",
      "The Gray Rhino: How to Recognize and Act on the Obvious Dangers We Ignore\n",
      "The Golden Condom: And Other Essays on Love Lost and Found\n",
      "The Epidemic (The Program 0.6)\n",
      "The Dinner Party\n",
      "The Diary of a Young Girl\n",
      "The Children\n",
      "Stars Above (The Lunar Chronicles #4.5)\n",
      "Snatched: How A Drug Queen Went Undercover for the DEA and Was Kidnapped By Colombian Guerillas\n",
      "Raspberry Pi Electronics Projects for the Evil Genius\n",
      "Quench Your Own Thirst: Business Lessons Learned Over a Beer or Two\n",
      "Psycho: Sanitarium (Psycho #1.5)\n",
      "Poisonous (Max Revere Novels #3)\n",
      "One with You (Crossfire #5)\n",
      "No Love Allowed (Dodge Cove #1)\n",
      "Murder at the 42nd Street Library (Raymond Ambler #1)\n",
      "Most Wanted\n",
      "Love, Lies and Spies\n",
      "How to Speak Golf: An Illustrated Guide to Links Lingo\n",
      "Hide Away (Eve Duncan #20)\n",
      "Furiously Happy: A Funny Book About Horrible Things\n",
      "Everyday Italian: 125 Simple and Delicious Recipes\n",
      "Equal Is Unfair: America's Misguided Fight Against Income Inequality\n",
      "Eleanor & Park\n",
      "Dirty (Dive Bar #1)\n",
      "Can You Keep a Secret? (Fear Street Relaunch #4)\n",
      "Boar Island (Anna Pigeon #19)\n",
      "A Paris Apartment\n",
      "A la Mode: 120 Recipes in 60 Pairings: Pies, Tarts, Cakes, Crisps, and More Topped with Ice Cream, Gelato, Frozen Custard, and More\n",
      "Troublemaker: Surviving Hollywood and Scientology\n",
      "The Widow\n",
      "The Sleep Revolution: Transforming Your Life, One Night at a Time\n",
      "The Improbability of Love\n",
      "The Art of Startup Fundraising\n",
      "Take Me Home Tonight (Rock Star Romance #3)\n",
      "Sleeping Giants (Themis Files #1)\n",
      "Setting the World on Fire: The Brief, Astonishing Life of St. Catherine of Siena\n",
      "Playing with Fire\n",
      "Off the Hook (Fishing for Trouble #1)\n",
      "Mothering Sunday\n",
      "Mother, Can You Not?\n",
      "M Train\n",
      "Lilac Girls\n",
      "Lies and Other Acts of Love\n",
      "Lab Girl\n",
      "Keep Me Posted\n",
      "It Didn't Start with You: How Inherited Family Trauma Shapes Who We Are and How to End the Cycle\n",
      "Grey (Fifty Shades #4)\n",
      "Exit, Pursued by a Bear\n",
      "Daredevils\n",
      "Cravings: Recipes for What You Want to Eat\n",
      "Born for This: How to Find the Work You Were Meant to Do\n",
      "Arena\n",
      "Adultery\n",
      "A Mother's Reckoning: Living in the Aftermath of Tragedy\n",
      "A Gentleman's Position (Society of Gentlemen #3)\n",
      "11/22/63\n",
      "10% Happier: How I Tamed the Voice in My Head, Reduced Stress Without Losing My Edge, and Found Self-Help That Actually Works\n",
      "10-Day Green Smoothie Cleanse: Lose Up to 15 Pounds in 10 Days!\n",
      "Without Shame\n",
      "Watchmen\n",
      "Unlimited Intuition Now\n",
      "Underlying Notes\n",
      "The Shack\n",
      "The New Brand You: Your New Image Makes the Sale for You\n",
      "The Moosewood Cookbook: Recipes from Moosewood Restaurant, Ithaca, New York\n",
      "The Flowers Lied\n",
      "The Fabric of the Cosmos: Space, Time, and the Texture of Reality\n",
      "The Book of Mormon\n",
      "The Art and Science of Low Carbohydrate Living\n",
      "The Alien Club\n",
      "Suzie Snowflake: One beautiful flake (a self-esteem story)\n",
      "Nap-a-Roo\n",
      "NaNo What Now? Finding your editing process, revising your NaNoWriMo book and building a writing career through publishing and beyond\n",
      "Modern Day Fables\n",
      "If I Gave You God's Phone Number....: Searching for Spirituality in America\n",
      "Fruits Basket, Vol. 9 (Fruits Basket #9)\n",
      "Dress Your Family in Corduroy and Denim\n",
      "Don't Forget Steven\n",
      "Chernobyl 01:23:40: The Incredible True Story of the World's Worst Nuclear Disaster\n",
      "Art and Fear: Observations on the Perils (and Rewards) of Artmaking\n",
      "A Shard of Ice (The Black Symphony Saga #1)\n",
      "A Hero's Curse (The Unseen Chronicles #1)\n",
      "23 Degrees South: A Tropical Tale of Changing Whether...\n",
      "Zero to One: Notes on Startups, or How to Build the Future\n",
      "Why Not Me?\n",
      "When Breath Becomes Air\n",
      "Vagabonding: An Uncommon Guide to the Art of Long-Term World Travel\n",
      "The Unlikely Pilgrimage of Harold Fry (Harold Fry #1)\n",
      "The New Drawing on the Right Side of the Brain\n",
      "The Midnight Assassin: Panic, Scandal, and the Hunt for America's First Serial Killer\n",
      "The Martian (The Martian #1)\n",
      "The High Mountains of Portugal\n",
      "The Grownup\n",
      "The E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do About It\n",
      "South of Sunshine\n",
      "Smarter Faster Better: The Secrets of Being Productive in Life and Business\n",
      "Silence in the Dark (Logan Point #4)\n",
      "Shadows of the Past (Logan Point #1)\n",
      "Roller Girl\n",
      "Rising Strong\n",
      "Proofs of God: Classical Arguments from Tertullian to Barth\n",
      "Please Kill Me: The Uncensored Oral History of Punk\n",
      "Out of Print: City Lights Spotlight No. 14\n",
      "My Life Next Door (My Life Next Door )\n",
      "Miller's Valley\n",
      "Man's Search for Meaning\n",
      "Love That Boy: What Two Presidents, Eight Road Trips, and My Son Taught Me About a Parent's Expectations\n",
      "Living Forward: A Proven Plan to Stop Drifting and Get the Life You Want\n",
      "Les Fleurs du Mal\n",
      "Left Behind (Left Behind #1)\n",
      "Kill 'Em and Leave: Searching for James Brown and the American Soul\n",
      "Kierkegaard: A Christian Missionary to Christians\n",
      "John Vassos: Industrial Design for Modern Life\n",
      "I'll Give You the Sun\n",
      "I Will Find You\n",
      "Hystopia: A Novel\n",
      "Howl and Other Poems\n",
      "History of Beauty\n",
      "Heaven is for Real: A Little Boy's Astounding Story of His Trip to Heaven and Back\n",
      "Future Shock (Future Shock #1)\n",
      "Ender's Game (The Ender Quintet #1)\n",
      "Diary of a Citizen Scientist: Chasing Tiger Beetles and Other New Ways of Engaging the World\n",
      "Death by Leisure: A Cautionary Tale\n",
      "Brilliant Beacons: A History of the American Lighthouse\n",
      "Brazen: The Courage to Find the You That's Been Hiding\n",
      "Between the World and Me\n",
      "Being Mortal: Medicine and What Matters in the End\n",
      "A Murder Over a Girl: Justice, Gender, Junior High\n",
      "32 Yolks\n",
      "\"Most Blessed of the Patriarchs\": Thomas Jefferson and the Empire of the Imagination\n",
      "You Are a Badass: How to Stop Doubting Your Greatness and Start Living an Awesome Life\n",
      "Wildlife of New York: A Five-Borough Coloring Book\n",
      "What Happened on Beale Street (Secrets of the South Mysteries #2)\n",
      "Unreasonable Hope: Finding Faith in the God Who Brings Purpose to Your Pain\n",
      "Under the Tuscan Sun\n",
      "Toddlers Are A**holes: It's Not Your Fault\n",
      "The Year of Living Biblically: One Man's Humble Quest to Follow the Bible as Literally as Possible\n",
      "The Whale\n",
      "The Story of Art\n",
      "The Origin of Species\n",
      "The Great Gatsby\n",
      "The Good Girl\n",
      "The Glass Castle\n",
      "The Faith of Christopher Hitchens: The Restless Soul of the World's Most Notorious Atheist\n",
      "The Drowning Girls\n",
      "The Constant Princess (The Tudor Court #1)\n",
      "The Bourne Identity (Jason Bourne #1)\n",
      "The Bachelor Girl's Guide to Murder (Herringford and Watts Mysteries #1)\n",
      "The Art Book\n",
      "The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change\n",
      "Team of Rivals: The Political Genius of Abraham Lincoln\n",
      "Steal Like an Artist: 10 Things Nobody Told You About Being Creative\n",
      "Sit, Stay, Love\n",
      "Sister Dear\n",
      "Shrunken Treasures: Literary Classics, Short, Sweet, and Silly\n",
      "Rich Dad, Poor Dad\n",
      "Raymie Nightingale\n",
      "Playing from the Heart\n",
      "Nightstruck: A Novel\n",
      "Naturally Lean: 125 Nourishing Gluten-Free, Plant-Based Recipes--All Under 300 Calories\n",
      "Meternity\n",
      "Memoirs of a Geisha\n",
      "Like Never Before (Walker Family #2)\n",
      "Life of Pi\n",
      "Leave This Song Behind: Teen Poetry at Its Best\n",
      "King's Folly (The Kinsman Chronicles #1)\n",
      "John Adams\n",
      "How to Cook Everything Vegetarian: Simple Meatless Recipes for Great Food (How to Cook Everything)\n",
      "How to Be a Domestic Goddess: Baking and the Art of Comfort Cooking\n",
      "Good in Bed (Cannie Shapiro #1)\n",
      "Fruits Basket, Vol. 7 (Fruits Basket #7)\n",
      "For the Love: Fighting for Grace in a World of Impossible Standards\n",
      "Finding God in the Ruins: How God Redeems Pain\n",
      "Every Heart a Doorway (Every Heart A Doorway #1)\n",
      "Delivering the Truth (Quaker Midwife Mystery #1)\n",
      "Counted With the Stars (Out from Egypt #1)\n",
      "Chronicles, Vol. 1\n",
      "Blue Like Jazz: Nonreligious Thoughts on Christian Spirituality\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in allPages :\n",
    "    response = requests.get(i)\n",
    "    html = response.text\n",
    "    data = BeautifulSoup(html,\"html.parser\")\n",
    "    book_title = data.find_all(class_ = \"product_pod\")\n",
    "    for b1 in book_title : \n",
    "        print(b1.h3.a[\"title\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 13 : Book Details**  \n",
    "Find and print the details of all books which are present on first 2 pages of this website.  \n",
    "All details include - Title of the book, book page url, Price (in float, without any currency or extra   symbol), and quantity in stock (in integer). Save all the details in a dataframe and print in the required format.  \n",
    "*Note:*    \n",
    "        \n",
    "        Remove the Trailing Zeros from price of book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html 51.77 22\n",
      "Tipping the Velvet http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html 53.74 20\n",
      "Soumission http://books.toscrape.com/catalogue/soumission_998/index.html 50.10 20\n",
      "Sharp Objects http://books.toscrape.com/catalogue/sharp-objects_997/index.html 47.82 20\n",
      "Sapiens: A Brief History of Humankind http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html 54.23 20\n",
      "The Requiem Red http://books.toscrape.com/catalogue/the-requiem-red_995/index.html 22.65 19\n",
      "The Dirty Little Secrets of Getting Your Dream Job http://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html 33.34 19\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull http://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html 17.93 19\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics http://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html 22.60 19\n",
      "The Black Maria http://books.toscrape.com/catalogue/the-black-maria_991/index.html 52.15 19\n",
      "Starving Hearts (Triangular Trade Trilogy, #1) http://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html 13.99 19\n",
      "Shakespeare's Sonnets http://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html 20.66 19\n",
      "Set Me Free http://books.toscrape.com/catalogue/set-me-free_988/index.html 17.46 19\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) http://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html 52.29 19\n",
      "Rip it Up and Start Again http://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html 35.02 19\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 http://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html 57.25 19\n",
      "Olio http://books.toscrape.com/catalogue/olio_984/index.html 23.88 19\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849 http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html 37.59 19\n",
      "Libertarianism for Beginners http://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html 51.33 19\n",
      "It's Only the Himalayas http://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html 45.17 19\n",
      "In Her Wake http://books.toscrape.com/catalogue/in-her-wake_980/index.html 12.84 19\n",
      "How Music Works http://books.toscrape.com/catalogue/how-music-works_979/index.html 37.32 19\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More http://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html 30.52 19\n",
      "Chase Me (Paris Nights #2) http://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html 25.27 19\n",
      "Black Dust http://books.toscrape.com/catalogue/black-dust_976/index.html 34.53 19\n",
      "Birdsong: A Story in Pictures http://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html 54.64 19\n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana http://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html 22.50 19\n",
      "Aladdin and His Wonderful Lamp http://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html 53.13 19\n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe http://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html 40.30 18\n",
      "Wall and Piece http://books.toscrape.com/catalogue/wall-and-piece_971/index.html 44.18 18\n",
      "The Four Agreements: A Practical Guide to Personal Freedom http://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html 17.66 18\n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate http://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html 31.05 18\n",
      "The Elephant Tree http://books.toscrape.com/catalogue/the-elephant-tree_968/index.html 23.82 18\n",
      "The Bear and the Piano http://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html 36.89 18\n",
      "Sophie's World http://books.toscrape.com/catalogue/sophies-world_966/index.html 15.94 18\n",
      "Penny Maybe http://books.toscrape.com/catalogue/penny-maybe_965/index.html 33.29 18\n",
      "Maude (1883-1993):She Grew Up with the country http://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html 18.02 18\n",
      "In a Dark, Dark Wood http://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html 19.63 18\n",
      "Behind Closed Doors http://books.toscrape.com/catalogue/behind-closed-doors_962/index.html 52.22 18\n",
      "You can't bury them all: Poems http://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html 33.63 17\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd \n",
    "allPages = ['http://books.toscrape.com/catalogue/page-1.html',\n",
    "            'http://books.toscrape.com/catalogue/page-2.html']\n",
    "\n",
    "column_names = ['Title', 'Link', 'Price', 'Quantity in Stock']\n",
    "\n",
    "all_books_url = []\n",
    "base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "for i in allPages:\n",
    "    response = requests.get(i)\n",
    "    html = response.text\n",
    "    data = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    for j in data.find_all(class_ = \"product_pod\"):\n",
    "        all_books_url.append(base_url+j.h3.a[\"href\"])\n",
    "\n",
    "\n",
    "\n",
    "values = []\n",
    "\n",
    "for link in all_books_url :\n",
    "    ## making the get request to the book link\n",
    "    response2 = requests.get(link)\n",
    "    html2 = response2.text\n",
    "    data = BeautifulSoup(html2,\"html.parser\")\n",
    "\n",
    "    ## getting the title of the book \n",
    "    title = data.h1.string\n",
    "\n",
    "    ## Price of the book \n",
    "    val  = data.find(class_ = \"price_color\").string\n",
    "    # print(val)\n",
    "\n",
    "    ## getting only the integer part from the sting of price \n",
    "    price = re.search(\"[\\d.]+\",val).group()\n",
    "\n",
    "    ## quantity of the book and geeting the interger part from the string \n",
    "    qty = data.find(class_ = \"instock availability\")\n",
    "    val  = qty.contents[-1].strip()\n",
    "    qty = re.search(\"\\d+\",val).group()\n",
    "    values.append([title,link,price,qty])\n",
    "\n",
    "\n",
    "# for i in values:\n",
    "#     print(*i)\n",
    "df  = pd.DataFrame(values,columns = column_names)\n",
    "df.to_csv(\"bookDetail.csv\")\n",
    "for i in range(len(df)):\n",
    "    print(df['Title'][i],df['Link'][i],df['Price'][i],df['Quantity in Stock'][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html 51.77 22 \n",
      "Tipping the Velvet http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html 53.74 20 \n",
      "Soumission http://books.toscrape.com/catalogue/soumission_998/index.html 50.1 20 \n",
      "Sharp Objects http://books.toscrape.com/catalogue/sharp-objects_997/index.html 47.82 20 \n",
      "Sapiens: A Brief History of Humankind http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html 54.23 20 \n",
      "The Requiem Red http://books.toscrape.com/catalogue/the-requiem-red_995/index.html 22.65 19 \n",
      "The Dirty Little Secrets of Getting Your Dream Job http://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html 33.34 19 \n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull http://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html 17.93 19 \n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics http://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html 22.6 19 \n",
      "The Black Maria http://books.toscrape.com/catalogue/the-black-maria_991/index.html 52.15 19 \n",
      "Starving Hearts (Triangular Trade Trilogy, #1) http://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html 13.99 19 \n",
      "Shakespeare's Sonnets http://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html 20.66 19 \n",
      "Set Me Free http://books.toscrape.com/catalogue/set-me-free_988/index.html 17.46 19 \n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) http://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html 52.29 19 \n",
      "Rip it Up and Start Again http://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html 35.02 19 \n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 http://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html 57.25 19 \n",
      "Olio http://books.toscrape.com/catalogue/olio_984/index.html 23.88 19 \n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849 http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html 37.59 19 \n",
      "Libertarianism for Beginners http://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html 51.33 19 \n",
      "It's Only the Himalayas http://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html 45.17 19 \n",
      "In Her Wake http://books.toscrape.com/catalogue/in-her-wake_980/index.html 12.84 19 \n",
      "How Music Works http://books.toscrape.com/catalogue/how-music-works_979/index.html 37.32 19 \n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More http://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html 30.52 19 \n",
      "Chase Me (Paris Nights #2) http://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html 25.27 19 \n",
      "Black Dust http://books.toscrape.com/catalogue/black-dust_976/index.html 34.53 19 \n",
      "Birdsong: A Story in Pictures http://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html 54.64 19 \n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana http://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html 22.5 19 \n",
      "Aladdin and His Wonderful Lamp http://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html 53.13 19 \n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe http://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html 40.3 18 \n",
      "Wall and Piece http://books.toscrape.com/catalogue/wall-and-piece_971/index.html 44.18 18 \n",
      "The Four Agreements: A Practical Guide to Personal Freedom http://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html 17.66 18 \n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate http://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html 31.05 18 \n",
      "The Elephant Tree http://books.toscrape.com/catalogue/the-elephant-tree_968/index.html 23.82 18 \n",
      "The Bear and the Piano http://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html 36.89 18 \n",
      "Sophie's World http://books.toscrape.com/catalogue/sophies-world_966/index.html 15.94 18 \n",
      "Penny Maybe http://books.toscrape.com/catalogue/penny-maybe_965/index.html 33.29 18 \n",
      "Maude (1883-1993):She Grew Up with the country http://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html 18.02 18 \n",
      "In a Dark, Dark Wood http://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html 19.63 18 \n",
      "Behind Closed Doors http://books.toscrape.com/catalogue/behind-closed-doors_962/index.html 52.22 18 \n",
      "You can't bury them all: Poems http://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html 33.63 17 \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "allPages = ['http://books.toscrape.com/catalogue/page-1.html',\n",
    "            'http://books.toscrape.com/catalogue/page-2.html']\n",
    "\n",
    "column_names = ['Title', 'Link', 'Price', 'Quantity in Stock']\n",
    "\n",
    "\n",
    "\n",
    "base_url=\"http://books.toscrape.com/catalogue/\"\n",
    "book_webpages=[]\n",
    "\n",
    "\n",
    "for i in allPages:\n",
    "    response=requests.get(i)\n",
    "    data=BeautifulSoup(response.text,\"html.parser\")\n",
    "    \n",
    "    for j in data.find_all(class_=\"product_pod\") :\n",
    "        book_webpages.append(base_url+j.h3.a[\"href\"])\n",
    "\n",
    "Title=[]\n",
    "Link=[]\n",
    "Price=[]\n",
    "Quantity=[] \n",
    "\n",
    "for link in book_webpages:\n",
    "    r=requests.get(link)\n",
    "    data=BeautifulSoup(r.text,\"html.parser\")\n",
    "    \n",
    "    tag=data.find(class_=\"col-sm-6 product_main\")\n",
    "    \n",
    "    Title.append(tag.h1.string.strip())\n",
    "    Link.append(link)\n",
    "    string=tag.p.string.strip()\n",
    "    Price.append(float(string[2:]))\n",
    "    l=list(tag.find(class_=\"instock availability\").strings)\n",
    "    s=l[1].strip()\n",
    "    Quantity.append(int(s[10:(len(s)-11)]))\n",
    "\n",
    "df=pd.DataFrame({\"title\":Title,\"link\":Link,\"price\":Price,\"quantity\":Quantity})\n",
    "\n",
    "for row in range(len(df.title)) :\n",
    "    ans=df.iloc[row,:].values\n",
    "    for i in ans:\n",
    "        print(i,end=\"\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1 : Print the data of first 3 movies**   \n",
    "From this https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt  \n",
    "Find and print the name and genre of the first 3 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War ; Action, Adventure, Sci-Fi\n",
      "Black Panther ; Action, Adventure, Sci-Fi\n",
      "Deadpool 2 ; Action, Adventure, Comedy\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt\")\n",
    "html = response.text\n",
    "data = BeautifulSoup(html,\"html.parser\")\n",
    "title = data.find_all(class_='lister-item-header')[:3]\n",
    "genre = data.find_all(class_='genre')[:3]\n",
    "for i in range(len(title)):\n",
    "    print(title[i].find('a').string+' ;',end=' ')\n",
    "    print(genre[i].string.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War ; Action, Adventure, Sci-Fi\n",
      "Black Panther ; Action, Adventure, Sci-Fi\n",
      "Deadpool 2 ; Action, Adventure, Comedy\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt\")\n",
    "html = response.text\n",
    "data = BeautifulSoup(html,\"html.parser\")\n",
    "title = data.find_all(class_ = \"lister-item-content\")[:3]\n",
    "for i in title :\n",
    "    # print(i.h3.contents[3].string,end = \" ; \") ##BOTH THE THINGS ARE CORRECT \n",
    "    print(i.h3.a.string,end = \" ; \")\n",
    "    print(i.p.find(class_ = \"genre\").string.strip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2 : titles with most votes**    \n",
    "Link to use https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt  \n",
    "Print the names of movies with highest number of votes from year 2010 to 2014  \n",
    "Note : Print the titles line wise starting from year 2010  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "Game of Thrones\n",
      "The Dark Knight Rises\n",
      "The Wolf of Wall Street\n",
      "Interstellar\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "### set the sort by number of votes on site than copy the link and make the changes in date accordingly in url \n",
    "urls = ['https://www.imdb.com/search/title/?release_date=2010-01-01,2010-12-31&sort=num_votes,desc&ref_=adv_prv', 'https://www.imdb.com/search/title/?release_date=2011-01-01,2011-12-31&sort=num_votes,desc&ref_=adv_prv',\n",
    "       'https://www.imdb.com/search/title/?release_date=2012-01-01,2012-12-31&sort=num_votes,desc&ref_=adv_prv', 'https://www.imdb.com/search/title/?release_date=2013-01-01,2013-12-31&sort=num_votes,desc&ref_=adv_prv',\n",
    "       'https://www.imdb.com/search/title/?release_date=2014-01-01,2014-12-31&sort=num_votes,desc&ref_=adv_prv']\n",
    "\n",
    "\n",
    "for i in urls:\n",
    "    res = requests.get(i)\n",
    "    data = BeautifulSoup(res.text,\"html.parser\")\n",
    "    name = data.find(class_ = \"lister-item-content\").a.string\n",
    "    print(name)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3 : title with maximum duration**  \n",
    "Link https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt  \n",
    "Out of the first 250 titles with highest number of votes in 2018,find which title has the maximum duration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Haunting of Hill House 572\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dct = {}\n",
    "for i in range(1,6):\n",
    "    response = requests.get(\"https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=\"+str(i)+\"&ref_=adv_nxt\")\n",
    "    html = response.text\n",
    "    data = BeautifulSoup(html,\"html.parser\")\n",
    "    tags = data.find_all(class_ = \"lister-item mode-advanced\")\n",
    "    # print(tags)\n",
    "    for j in tags :\n",
    "        # print(j)\n",
    "        ### i use the walrus operator := please read the article on google regarding this \n",
    "        if val := j.find('span',class_='runtime'):   \n",
    "            head = j.find('h3',class_='lister-item-header')\n",
    "            dur = val\n",
    "            time = int(dur.string.strip().split(\" \")[0])\n",
    "            dct[head.a.string] = time \n",
    "\n",
    "maxdur = -1\n",
    "maxname = \"randome_string\"\n",
    "# l = dct.keys()\n",
    "for k,v in dct.items():\n",
    "    if v>maxdur:\n",
    "        maxdur = v\n",
    "        maxname = k\n",
    "print(maxname,maxdur)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4 : Image with maximum area**  \n",
    "From this website : https://en.wikipedia.org/wiki/Artificial_intelligence  \n",
    "Find and print the src of the < img > tag which occupies the maximum area on the page.  \n",
    "*Note :*\n",
    "Ignore images which doesn't have height or width attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "data = BeautifulSoup(response.text,\"html.parser\")\n",
    "all_tag = data.find_all('img')\n",
    "max_area = -1\n",
    "url = ''\n",
    "for i in all_tag :\n",
    "    ### check tag i having the height and width attribute \n",
    "    if i.has_attr('height') and i.has_attr('width'):\n",
    "        if int(i.attrs['height']) * int(i.attrs['width']) > max_area:\n",
    "            max_area = int(i['height']) * int(i['width'])\n",
    "            url = i.attrs['src']\n",
    "print(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRoblem 5: Quotes with tag humor**   \n",
    "Find all the quotes that have the tag as \"humor\" from https://quotes.toscrape.com website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“A day without sunshine is like, you know, night.”\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
      "“Some people never go crazy. What truly horrible lives they must lead.”\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
      "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
      "“I am free of all prejudice. I hate everyone equally. ”\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://quotes.toscrape.com\")\n",
    "base_url = \"https://quotes.toscrape.com\"\n",
    "t_list = []\n",
    "while response.status_code == 200 :\n",
    "    data = BeautifulSoup(response.text,\"html.parser\")\n",
    "    tags = data.find_all(\"div\",class_ = \"quote\")\n",
    "    for i in tags :\n",
    "        temp_tag = []\n",
    "        tag_list = i.find_all(\"a\",class_ = \"tag\")\n",
    "        t_quote = i.find(\"span\",class_ = \"text\").string\n",
    "        for j in tag_list :\n",
    "            temp_tag.append(j.string)\n",
    "        if 'humor' in temp_tag:\n",
    "            t_list.append(t_quote)\n",
    "    next_page= data.find(class_='next')\n",
    "    if next_page is None :\n",
    "        break \n",
    "    next_url = base_url+next_page.a[\"href\"]\n",
    "    response = requests.get(next_url)\n",
    "\n",
    "\n",
    "for _ in t_list :\n",
    "    print(_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6 :Print all authors**    \n",
    "Find and print the names of all the different authors from all pages of https://quotes.toscrape.com website  \n",
    "Note : Print the names of all authors line wise sorted in dictionary order  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n",
      "Alexandre Dumas fils\n",
      "Alfred Tennyson\n",
      "Allen Saunders\n",
      "André Gide\n",
      "Ayn Rand\n",
      "Bob Marley\n",
      "C.S. Lewis\n",
      "Charles Bukowski\n",
      "Charles M. Schulz\n",
      "Douglas Adams\n",
      "Dr. Seuss\n",
      "E.E. Cummings\n",
      "Eleanor Roosevelt\n",
      "Elie Wiesel\n",
      "Ernest Hemingway\n",
      "Friedrich Nietzsche\n",
      "Garrison Keillor\n",
      "George Bernard Shaw\n",
      "George Carlin\n",
      "George Eliot\n",
      "George R.R. Martin\n",
      "Harper Lee\n",
      "Haruki Murakami\n",
      "Helen Keller\n",
      "J.D. Salinger\n",
      "J.K. Rowling\n",
      "J.M. Barrie\n",
      "J.R.R. Tolkien\n",
      "James Baldwin\n",
      "Jane Austen\n",
      "Jim Henson\n",
      "Jimi Hendrix\n",
      "John Lennon\n",
      "Jorge Luis Borges\n",
      "Khaled Hosseini\n",
      "Madeleine L'Engle\n",
      "Marilyn Monroe\n",
      "Mark Twain\n",
      "Martin Luther King Jr.\n",
      "Mother Teresa\n",
      "Pablo Neruda\n",
      "Ralph Waldo Emerson\n",
      "Stephenie Meyer\n",
      "Steve Martin\n",
      "Suzanne Collins\n",
      "Terry Pratchett\n",
      "Thomas A. Edison\n",
      "W.C. Fields\n",
      "William Nicholson\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://quotes.toscrape.com\")\n",
    "base_url = \"https://quotes.toscrape.com\"\n",
    "t_dct = {}\n",
    "while response.status_code == 200 :\n",
    "    data = BeautifulSoup(response.text,\"html.parser\")\n",
    "    author_list = data.find_all(\"small\",class_ = \"author\")\n",
    "    for _ in author_list :\n",
    "        t_dct[_.string] = t_dct.get(_.string,0)+1\n",
    "    next_page = data.find(class_='next')\n",
    "    if next_page is None :\n",
    "        break \n",
    "    next_url = base_url+next_page.a[\"href\"]\n",
    "    response = requests.get(next_url)\n",
    "\n",
    "l = sorted(list(t_dct.keys()))\n",
    "for _ in l :\n",
    "    print(_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7 :Birth Date of authors**   \n",
    "Find the birth date of authors whose name start with 'J' from http://quotes.toscrape.com website  \n",
    "Note : Print a dictionary containing the name as key and the birth date as value.The Names of authors should  be alphabetically sorted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J.D. Salinger': 'January 01, 1919', 'J.K. Rowling': 'July 31, 1965', 'J.M. Barrie': 'May 09, 1860', 'J.R.R. Tolkien': 'January 03, 1892', 'James Baldwin': 'August 02, 1924', 'Jane Austen': 'December 16, 1775', 'Jim Henson': 'September 24, 1936', 'Jimi Hendrix': 'November 27, 1942', 'John Lennon': 'October 09, 1940', 'Jorge Luis Borges': 'August 24, 1899'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "authors = {}\n",
    "for i in range(1,11):\n",
    "    response = requests.get('http://quotes.toscrape.com/page/'+ str(i) + '/')\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    for aut in data.select('.author') :\n",
    "        if aut.text[0] == 'J':\n",
    "            authors[aut.text] = aut.next_sibling.next_sibling['href']\n",
    "bdate ={}\n",
    "for author in sorted(authors) :\n",
    "    page = requests.get('http://quotes.toscrape.com'+ authors[author])\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    i = data.find(\"span\",class_ = 'author-born-date')\n",
    "    bdate[author] = i.text\n",
    "print(bdate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem : 8 Quotes by Albert Einstein**  \n",
    "Find all the quotes by Albert Einstein(in the order they appear on the page) from http://quotes.toscrape.com website  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "“Any fool can know. The point is to understand.”\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
      "“Anyone who has never made a mistake has never tried anything new.”\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://quotes.toscrape.com\")\n",
    "base_url = \"https://quotes.toscrape.com\"\n",
    "while response.status_code == 200 :\n",
    "    data = BeautifulSoup(response.text,\"html.parser\")\n",
    "    tags = data.find_all(\"div\",class_ = \"quote\") \n",
    "    for i in tags :\n",
    "        author_name = i.find(\"small\",class_ = \"author\").string\n",
    "        if author_name == \"Albert Einstein\":\n",
    "            print(i.find(\"span\",class_ = \"text\").string)\n",
    "\n",
    "    next_page= data.find(class_='next')\n",
    "    if next_page is None :\n",
    "        break \n",
    "    next_url = base_url+next_page.a[\"href\"]\n",
    "    response = requests.get(next_url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9 : First 5 Blogs** \n",
    "Link to use https://medium.com/codingninjas-blog  \n",
    "Print the title of the first 5 blogs written by Coding Ninjas  \n",
    "Note : Print the blog names line wise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Development Trends to watch out for in 2020\n",
      "Web Development: Interviews and You!\n",
      "Get equipped for the Technical Interviews\n",
      "Explore more about the projects in Web Development\n",
      "5G to be a major gamechanger for Edu-tech platforms\n",
      "Web Development Trends to watch out for in 2020\n",
      "Web Development: Interviews and You!\n",
      "Get equipped for the Technical Interviews\n",
      "Explore more about the projects in Web Development\n",
      "5G to be a major gamechanger for Edu-tech platforms\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://medium.com/codingninjas-blog\")\n",
    "data = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "tags = data.find_all(class_ = \"section-content\")[:5]\n",
    "\n",
    "for _ in tags :\n",
    "    print(_.h3.string)\n",
    "\n",
    "print('''Web Development Trends to watch out for in 2020\n",
    "Web Development: Interviews and You!\n",
    "Get equipped for the Technical Interviews\n",
    "Explore more about the projects in Web Development\n",
    "5G to be a major gamechanger for Edu-tech platforms''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
